#+title: The Bottleneck Simulator: A Model-Based Deep Reinforcement Learning Approach
#+author: Serban, I. V., Sankar, C., Pieper, M., Pineau, J., & Bengio, Y.
#+include: "head.org"
#+LINK: img file:img/%s
#+LINK: png file:img/%s.png
#+LINK: jpg file:img/%s.jpg
#+LINK: spng file:img/static/%s.png
#+LINK: sjpg file:img/static/%s.jpg

#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+options: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+options: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t latex:t timestamp:t title:t toc:nil todo:t |:t
#+title: presen
#+date: <2021-07-20 ç«>
#+author: masataro
#+email: guicho2.71828@gmail.com
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 27.2 (Org mode 9.4.4)

#+html_head: <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:500,900">
#+html_head_extra:

* Bottleneck simulator

[[png:graphical]]

* Transition model learning

Maximize $\log p(s'|s,a)$ where

\[
p(s'|s,a)=\sum_z p(s|z)p(z|s,a)
\]

However,

\[
p(z|s)= \left\{\begin{array} 1 & f_{s\rightarrow z}(s) = z \\ 0 & f_{s\rightarrow z}(s) \not= z \end{array} \right.
\]

Therefore 

\[
p(s'|s,a)= p(s|f_{s\rightarrow z}(s))p(f_{s\rightarrow z}(s)|s,a)
\]

Q-learning is performed based on this model

* Why not this model?

[[png:question1]]

* Why not this model?

[[png:question2]]

* Why not this model?

[[png:question3]]

